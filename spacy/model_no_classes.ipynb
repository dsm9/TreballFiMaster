{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model incidència 'No ha impartit classe a aquest grup'\n",
    "\n",
    "This notebook creates a evaluates a model to detect the issue 'No ha impartit classe a aquest grup'.\n",
    "\n",
    "It will be created a model to each language: catalan, spanish and english.\n",
    "\n",
    "Only are treated comments of type 'P : Professor'.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "#nlp = spacy.load(\"ca_fasttext_wiki\")\n",
    "#nlp = spacy.load(\"es_core_news_sm\")\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads information from preprocessed file to create de train and test data\n",
    "\n",
    "def load_data(limit = 0, split = 0.8, language = \"ca\"):\n",
    "\n",
    "    if (debug >= 1):\n",
    "        print (\"LOAD_DATA\")\n",
    "        \n",
    "    # Load data from file\n",
    "    file = \"comentaris_\" + language + \".csv\"\n",
    "    data = pd.read_csv(pathdest + file)\n",
    "    if (debug >= 2):\n",
    "        print (\"Original data:\")\n",
    "        display (data.sample(5))\n",
    "\n",
    "    # Calculates label and filter rows to be used    \n",
    "    data_prof = data[data.TipusPregunta == \"P\"][[\"Comentari\",\"TipusIncidencia\"]]\n",
    "    if (debug >= 2):\n",
    "        print (\"Filtered data:\")\n",
    "        display (data_prof.sample(5))    \n",
    "\n",
    "    # Calculates tuples row\n",
    "    # Converts: label=True -> {\"POSITIVE\": True, \"NEGATIVE\": False}\n",
    "    # label=False -> {\"POSITIVE\": False, \"NEGATIVE\": True}\n",
    "\n",
    "    data_prof[\"label\"] = data_prof[\"TipusIncidencia\"] == \"No ha impartit classe a aquest grup\"\n",
    "    data_prof[\"tuples\"] = data_prof.apply(lambda row: (row[\"Comentari\"], {\"POSITIVE\": bool(row[\"label\"]), \"NEGATIVE\": not bool(row[\"label\"])}), axis=1)    \n",
    "    if (debug >= 2):\n",
    "        print (\"Tuples dataframe:\")\n",
    "        display (data_prof.sample(5))\n",
    "\n",
    "    # Converts dataframe into list\n",
    "    train_data = data_prof[\"tuples\"].tolist()   \n",
    "    if (debug >= 2):\n",
    "        print (\"Tuples list:\")\n",
    "        print (train_data[:5])    \n",
    "\n",
    "    # Takes an aleatori set of tuples\n",
    "    random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    if (debug >= 2):\n",
    "        print (\"Shuffled tuples:\")\n",
    "        print (train_data[:5])    \n",
    "\n",
    "    # Split text and label into two lists\n",
    "    texts, cats = zip(*train_data)\n",
    "    if (debug >= 1):\n",
    "        print (\"Texts:\")\n",
    "        print (texts[0:5])\n",
    "        print (\"Cats:\")\n",
    "        print (cats[0:5])\n",
    "\n",
    "    # Size of train_data and test_data\n",
    "    split = int(len(train_data) * split)\n",
    "    if (debug >= 1):\n",
    "        print (\"Train data:\", split, \"Test data: \", len(train_data)-split)   \n",
    "        print (\"\")\n",
    "    \n",
    "    # Return train data and test data\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if label == \"NEGATIVE\":\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.0\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.0\n",
    "                if (debug >= 2):\n",
    "                    print (\"fp: \", doc)\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "                if (debug >= 2):\n",
    "                    print (\"fn: \", doc)                \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model=None, language=\"ca\", n_iter=20, n_texts=2000):\n",
    "    \n",
    "    # Load the model form spacy\n",
    "    nlp = spacy.load(model)\n",
    "\n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    # add label to text classifier\n",
    "    textcat.add_label(\"POSITIVE\")\n",
    "    textcat.add_label(\"NEGATIVE\")\n",
    "\n",
    "    # load training and test data\n",
    "    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(0, 0.8, language)\n",
    "    train_texts = train_texts[:n_texts]\n",
    "    train_cats = train_cats[:n_texts]\n",
    "\n",
    "    if debug:\n",
    "        print(\n",
    "            \"Using {} examples ({} training, {} evaluation)\".format(\n",
    "                n_texts, len(train_texts), len(dev_texts)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # converts the data to the format:\n",
    "    # (text, {'cats': {'POSITIVE': True, 'NEGATIVE': False}}))\n",
    "    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
    "    if debug:\n",
    "        print (\"Train_data: \")\n",
    "        print (train_data[:5])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"textcat\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "#        if init_tok2vec is not None:\n",
    "#            with init_tok2vec.open(\"rb\") as file_:\n",
    "#                textcat.model.tok2vec.from_bytes(file_.read())\n",
    "        print(\"Training the model...\")\n",
    "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "        batch_sizes = compounding(4.0, 32.0, 1.001)\n",
    "        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                if (debug >= 2):\n",
    "                    print (\"Batch: \")\n",
    "                    print (batch)\n",
    "                texts, annotations = zip(*batch)\n",
    "\n",
    "                # Eliminación del 20% de los casos para evitar generalizaciones\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print(\n",
    "                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(  # print a simple table\n",
    "                    losses[\"textcat\"],\n",
    "                    scores[\"textcat_p\"],\n",
    "                    scores[\"textcat_r\"],\n",
    "                    scores[\"textcat_f\"],\n",
    "                )\n",
    "            )  \n",
    "           \n",
    "    # test the trained model\n",
    "    print (\"\")\n",
    "    print (\"Some examples: \")\n",
    "    test_texts = [\"Costa que pengi els materials acordats al campus\",\n",
    "        \"No he tingut aquesta professora\",\n",
    "        \"No ha impartit classe a aquest grup\",\n",
    "        \"No ha corregit examens fets a octubre, i estem al mes de gener.\"]\n",
    "    for test_text in test_texts:\n",
    "        doc = nlp(test_text)\n",
    "        print(test_text, doc.cats)\n",
    "    print (\"\")\n",
    "    \n",
    "    # Save de model\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(pathmodel + language)\n",
    "        print(\"Saved model to\", pathmodel + language)\n",
    "        \n",
    "        # test the saved model\n",
    "        print(\"Loading from\", pathmodel+ language)\n",
    "        nlp2 = spacy.load(pathmodel +  language)\n",
    "        for test_text in test_texts:\n",
    "            doc2 = nlp2(test_text)\n",
    "            print(test_text, doc2.cats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathori = \"../data/original\"\n",
    "pathdest = \"../data/preprocessed/\"\n",
    "pathmodel = \"../data/processed/\"\n",
    "debug = 1\n",
    "\n",
    "language = \"ca\"\n",
    "model = \"ca_fasttext_wiki\"\n",
    "n_iter = 5\n",
    "n_texts = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD_DATA\n",
      "Texts:\n",
      "(\"Les correccions dels treballs no són constructivistes, al contrari, les rebem com una crítica sense opció a millora quan no s'ha donat una correcta comunicació entre docent i alumne/a.\\r\\nPel que fa a l'examen, no trobo coherent la manera de preguntar de l'examen amb els continguts dels Power Points que sovint es queden incomplets pel que ella demana.\", 'Explica la matèria una mica desorganitzada, però és bona professora. ', 'Posa moltes facilitats per poder compaginar la seva assignatura amb la feina. És molt propera i molt pacient.', \"No hem pogut fer gaires classes per a fer un comentari realista, però amb comparació amb l'altre professor, aquest explica molt millor i fa molts exemples per a que ho entenguem. També ens respon tots els dubtes.\", \"No sap explicar l'assignatura i no sap resoldre els exercicis que ha elaborat ell mateix.\\r\\nDiu merèixer respecte per la seva trajectòria professional.\\r\\nQuan se li comenta alguna opció relativa als exàmens per mirar d'adaptar-nos tots a la nova manera de fer les classes no hi posa cap voluntat per la seva part.\")\n",
      "Cats:\n",
      "({'POSITIVE': False, 'NEGATIVE': True}, {'POSITIVE': False, 'NEGATIVE': True}, {'POSITIVE': False, 'NEGATIVE': True}, {'POSITIVE': False, 'NEGATIVE': True}, {'POSITIVE': False, 'NEGATIVE': True})\n",
      "Train data: 2690 Test data:  673\n",
      "\n",
      "Using 2000 examples (2000 training, 673 evaluation)\n",
      "Train_data: \n",
      "[(\"Les correccions dels treballs no són constructivistes, al contrari, les rebem com una crítica sense opció a millora quan no s'ha donat una correcta comunicació entre docent i alumne/a.\\r\\nPel que fa a l'examen, no trobo coherent la manera de preguntar de l'examen amb els continguts dels Power Points que sovint es queden incomplets pel que ella demana.\", {'cats': {'POSITIVE': False, 'NEGATIVE': True}}), ('Explica la matèria una mica desorganitzada, però és bona professora. ', {'cats': {'POSITIVE': False, 'NEGATIVE': True}}), ('Posa moltes facilitats per poder compaginar la seva assignatura amb la feina. És molt propera i molt pacient.', {'cats': {'POSITIVE': False, 'NEGATIVE': True}}), (\"No hem pogut fer gaires classes per a fer un comentari realista, però amb comparació amb l'altre professor, aquest explica molt millor i fa molts exemples per a que ho entenguem. També ens respon tots els dubtes.\", {'cats': {'POSITIVE': False, 'NEGATIVE': True}}), (\"No sap explicar l'assignatura i no sap resoldre els exercicis que ha elaborat ell mateix.\\r\\nDiu merèixer respecte per la seva trajectòria professional.\\r\\nQuan se li comenta alguna opció relativa als exàmens per mirar d'adaptar-nos tots a la nova manera de fer les classes no hi posa cap voluntat per la seva part.\", {'cats': {'POSITIVE': False, 'NEGATIVE': True}})]\n",
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "0.576\t1.000\t0.250\t0.400\n",
      "0.100\t1.000\t0.250\t0.400\n",
      "0.020\t1.000\t0.250\t0.400\n",
      "0.004\t1.000\t0.250\t0.400\n",
      "0.002\t1.000\t0.250\t0.400\n",
      "\n",
      "Some examples: \n",
      "Costa que pengi els materials acordats al campus {'POSITIVE': 3.852297595585696e-06, 'NEGATIVE': 0.9999961853027344}\n",
      "No he tingut aquesta professora {'POSITIVE': 0.9999371767044067, 'NEGATIVE': 6.279721128521487e-05}\n",
      "No ha impartit classe a aquest grup {'POSITIVE': 0.9850050210952759, 'NEGATIVE': 0.014994950033724308}\n",
      "No ha corregit examens fets a octubre, i estem al mes de gener. {'POSITIVE': 1.5060870282468386e-05, 'NEGATIVE': 0.9999849796295166}\n",
      "\n",
      "Saved model to ../data/processed/ca\n",
      "Loading from ../data/processed/ca\n",
      "Costa que pengi els materials acordats al campus {'POSITIVE': 2.9023590286669787e-06, 'NEGATIVE': 0.9999971389770508}\n",
      "No he tingut aquesta professora {'POSITIVE': 0.999870777130127, 'NEGATIVE': 0.00012924290786031634}\n",
      "No ha impartit classe a aquest grup {'POSITIVE': 0.9103022217750549, 'NEGATIVE': 0.08969780802726746}\n",
      "No ha corregit examens fets a octubre, i estem al mes de gener. {'POSITIVE': 2.3340067855315283e-05, 'NEGATIVE': 0.999976634979248}\n"
     ]
    }
   ],
   "source": [
    "train_model (model, language, n_iter, n_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
