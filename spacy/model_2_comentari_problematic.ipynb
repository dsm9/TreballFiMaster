{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Tipus incidència 'Comentari problemàtic'\n",
    "\n",
    "This notebook creates a evaluates a model to detect the issue 'Comentari problemàtic'.\n",
    "\n",
    "It will be created a model to each language: catalan, spanish and english.\n",
    "\n",
    "This type of issue is characterized by long coments composed for some sentences and imprecisse expressions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "#nlp = spacy.load(\"ca_fasttext_wiki\")\n",
    "#nlp = spacy.load(\"es_core_news_sm\")\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates tuples with texts and cats in the whole comment\n",
    "\n",
    "def create_texts_and_cats_comments(data_com, nlp):\n",
    "    \n",
    "    # Converts data frame to list\n",
    "    #data_com = data_com.dropna(subset=\"Comentari\")\n",
    "    data_com = data_com[data_com[\"Comentari\"].notnull()]\n",
    "    data_list = data_com[\"tuples\"].tolist()\n",
    "    \n",
    "    # Change order of comments\n",
    "    random.shuffle(data_list)\n",
    "    if (debug >= 2):\n",
    "        print (\"Tuples list:\")\n",
    "        print (data_list[:5])  \n",
    "        \n",
    "    # Split text and label of true cases into two lists\n",
    "    texts, cats = zip(*data_list)\n",
    "    if (debug >= 2):\n",
    "        print (\"Texts:\")\n",
    "        print (texts[0:5])\n",
    "        print (\"Cats:\")\n",
    "        print (cats[0:5])  \n",
    "\n",
    "    return (texts, cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates tuples with texts and cats, spliting sentences in the comment\n",
    "\n",
    "def create_texts_and_cats_sentences(data_com, nlp):\n",
    "    \n",
    "    # Split comments in sentences\n",
    "    data_com = data_com[data_com[\"Comentari\"].notnull()]\n",
    "    train_data = data_com[\"tuples\"].tolist()\n",
    "    train_list = []\n",
    "    for row in train_data:\n",
    "        comment = row[0]\n",
    "        label = row[1]\n",
    "        doc = nlp(comment)\n",
    "        for sent in doc.sents:\n",
    "            train_list.append((sent.text, label))\n",
    "            if ((debug >=2) and (len(train_list)<=10)):\n",
    "                print(\"+ \", sent)\n",
    "                print(\"  \", label)\n",
    " \n",
    "    if (debug >= 1):\n",
    "        print(\"Sentences: \", len(train_list))\n",
    "\n",
    "    # Change order of comments\n",
    "    random.shuffle(train_list)\n",
    "    if (debug >= 2):\n",
    "        print (\"Tuples list:\")\n",
    "        print (train_list[:5])  \n",
    "        \n",
    "     # Split text and label of true cases into two lists\n",
    "    texts, cats = zip(*train_list)\n",
    "    if (debug >= 2):\n",
    "        print (\"Texts:\")\n",
    "        print (texts[0:5])\n",
    "        print (\"Cats:\")\n",
    "        print (cats[0:5])   \n",
    "        \n",
    "    return (texts, cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads information from preprocessed file to create de train and test data\n",
    "\n",
    "def load_data(limit = 0, split = 0.8, language = \"es\", nlp = None):\n",
    "\n",
    "    if (debug >= 1):\n",
    "        print (\"LOAD_DATA\")\n",
    "        print (\"Language: \", language)\n",
    "        \n",
    "    # Load the model form spacy\n",
    "#    nlp = spacy.load(model)\n",
    "#    if (debug >= 1):\n",
    "#        print (\"Model loaded: \", model)\n",
    "    \n",
    "    # Load data from file\n",
    "    file = \"comentaris_\" + language + \".csv\"\n",
    "    data = pd.read_csv(pathdest + file)\n",
    "    if (debug >= 1):\n",
    "        print(\"Original data: \", data.shape[0])\n",
    "        \n",
    "    if (debug >= 2):\n",
    "        print (\"Sample:\")\n",
    "        display (data.sample(5))\n",
    "\n",
    "    # Calculates label    \n",
    "    data_com = data[[\"Comentari\",\"TipusIncidencia\"]]    # CHANGED\n",
    "    if (debug >= 2):\n",
    "        print (\"Filtered columns:\")\n",
    "        display (data_com.sample(5))    \n",
    "\n",
    "    # Calculates tuples row\n",
    "    # Converts: label=True -> {\"POSITIVE\": True, \"NEGATIVE\": False}\n",
    "    # label=False -> {\"POSITIVE\": False, \"NEGATIVE\": True}\n",
    "    data_com[\"label\"] = data_com[\"TipusIncidencia\"] == \"Comentari problemàtic\"  # CHANGED\n",
    "    data_com[\"tuples\"] = data_com.apply(lambda row: (row[\"Comentari\"], {\"POSITIVE\": bool(row[\"label\"]), \"NEGATIVE\": not bool(row[\"label\"])}), axis=1)    \n",
    "    if (debug >= 2):\n",
    "        print (\"Tuples dataframe:\")\n",
    "        display (data_com.sample(5))\n",
    "\n",
    "    # Create texts and cats lists for true cases\n",
    "    data_true = data_com[data_com[\"label\"]==True]\n",
    "    if (debug >= 1):\n",
    "        print (\"True comments: \", data_true.shape[0])\n",
    "        \n",
    "    (texts_true, cats_true) = create_texts_and_cats_comments(data_true, nlp)\n",
    "    #(texts_true, cats_true) = create_texts_and_cats_sentences(data_true, nlp)\n",
    "    if (debug >= 2):\n",
    "        print(\"Texts:\",len(texts_true), \"Cats:\", len(cats_true))\n",
    "        \n",
    "    # Size of train_data_true and test_data_true\n",
    "    split_true = int(len(texts_true) * split)+1\n",
    "    if (debug >= 1):\n",
    "        print (\"Train True data:\", split_true, \", Test True data: \", len(texts_true)-split_true)      \n",
    "                \n",
    "    # Create texts and cats lists for false cases        \n",
    "    data_false = data_com[data_com[\"label\"]==False]\n",
    "    if (debug >= 1):\n",
    "        print (\"False comments:\", data_false.shape[0])\n",
    "\n",
    "    (texts_false, cats_false) = create_texts_and_cats_comments(data_false, nlp)\n",
    "    #(texts_false, cats_false) = create_texts_and_cats_sentences(data_false, nlp)\n",
    "    if (debug >= 2):\n",
    "        print(\"Texts:\",len(texts_false), \"Cats:\", len(cats_false))        \n",
    "       \n",
    "    # Size of train_data and test_data, considering limit\n",
    "    if (limit > len(texts_true) + len(texts_false)):\n",
    "        limit = len(texts_true) + len(texts_false)\n",
    "    split_total = int(limit * split)\n",
    "\n",
    "    if (debug >= 1):\n",
    "        print (\"Used data:\", limit)\n",
    "        print (\"Train data: \", split_total, \", Test data: \", limit - split_total)\n",
    "         \n",
    "    # Size of train_data_false and test_data_false\n",
    "    train_split_false = split_total - split_true\n",
    "    test_split_false = (limit - split_total) - (len(texts_true) - split_true) \n",
    "        \n",
    "    if (debug >= 1):\n",
    "        print (\"Used False comments: \", limit - len(texts_true))\n",
    "        print (\"Train False data: \", train_split_false, \", Test False data: \", test_split_false)  \n",
    "        \n",
    "         \n",
    "    # Mix true and false cases and split in train and devel\n",
    "    # All the true cases are included\n",
    "    train_texts = texts_true[:split_true] + texts_false[:train_split_false]\n",
    "    train_cats = cats_true[:split_true] + cats_false[:train_split_false]\n",
    "    test_texts = texts_true[split_true:] + texts_false[-test_split_false:]\n",
    "    test_cats = cats_true[split_true:] + cats_false[-test_split_false:]\n",
    "        \n",
    "    # Return train data and test data\n",
    "    return (train_texts, train_cats), (test_texts, test_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the level of accuracy using the test set to compare the prediction with the label defined by the user \n",
    "# Obtains the indicators: Precission (p), Recall (r) and F-score (f)\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    fp_list = list()\n",
    "    fn_list = list()\n",
    "    \n",
    "    if (debug == 2):\n",
    "        print (\"EVALUATE\")\n",
    "        \n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if label == \"NEGATIVE\":\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.0\n",
    "                if (debug >= 2):\n",
    "                    print (\"tp: \", doc)\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.0\n",
    "                fp_list.append(doc)\n",
    "                if (debug >= 2):\n",
    "                    print (\"fp: \", doc)\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "                if (debug >= 2):\n",
    "                    print (\"tn: \", doc)\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "                fn_list.append(doc)\n",
    "                if (debug >= 2):\n",
    "                    print (\"fn: \", doc)   \n",
    "    if (debug >= 2):\n",
    "        print(\"tp: \", tp, \", fp: \", fp, \", tn: \", tn, \", fn: \", fn)\n",
    "        \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score, \n",
    "            \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn,\n",
    "            \"fp_list\": fp_list, \"fn_list\": fn_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the training information, transforms it, trains the model with this set of data, evaluate the result \n",
    "# and saves the resulting model\n",
    "\n",
    "def train_model(model=None, language=\"ca\", target_loss=0.001, n_texts=2000):\n",
    "    if (debug >= 1):\n",
    "        print (\"TRAIN MODEL\")\n",
    "        \n",
    "    # Load the model form spacy\n",
    "    nlp = spacy.load(model)\n",
    "    if (debug >= 1):\n",
    "        print (\"Model loaded: \", model)\n",
    "        \n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    # add label to text classifier\n",
    "    textcat.add_label(\"POSITIVE\")\n",
    "    textcat.add_label(\"NEGATIVE\")\n",
    "    \n",
    "    if \"sentencizer\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "        \n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"textcat\", \"trf_wordpiecer\", \"trf_tok2vec\",\"sentencizer\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "#        if init_tok2vec is not None:\n",
    "#            with init_tok2vec.open(\"rb\") as file_:\n",
    "#                textcat.model.tok2vec.from_bytes(file_.read())        \n",
    "        \n",
    "        if (debug >= 1):\n",
    "            print (\"Pipes prepared\")\n",
    "        \n",
    "        # load training and test data\n",
    "        (train_texts, train_cats), (dev_texts, dev_cats) = load_data(n_texts, 0.8, language, nlp)        \n",
    "\n",
    "        if (debug >= 1):\n",
    "            print(\n",
    "                \"Using {} examples ({} training, {} evaluation)\".format(\n",
    "                    len(train_texts)+len(dev_texts), len(train_texts), len(dev_texts)\n",
    "                )\n",
    "            )      \n",
    "\n",
    "        # converts the data to the format:\n",
    "        # (text, {'cats': {'POSITIVE': True, 'NEGATIVE': False}}))\n",
    "        train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
    "        if (debug>=1):\n",
    "            print (\"Train_data: \")\n",
    "            print (train_data[:5])        \n",
    "    \n",
    "        # Train the model\n",
    "        print(\"Training the model...\")\n",
    "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\n",
    "            \"ITER\", \"LOSS\", \"P\", \"R\", \"F\",\"TP\",\"FP\",\"FN\",\"TN\"))\n",
    "        batch_sizes = compounding(4.0, 32.0, 1.001)\n",
    "        loss = 100\n",
    "        prev_loss = 1000\n",
    "        iter = 1\n",
    "#        if (loss > target_loss):\n",
    "        while ((loss > target_loss) | (prev_loss - loss > target_loss)) & (iter <= 20):\n",
    "            prev_loss = loss\n",
    "    #        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=batch_sizes)\n",
    "\n",
    "#            if True:\n",
    "#                batch = next(batches)\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                if (debug >= 2):\n",
    "                    print (\"Batch: \", len(batch))\n",
    "                    print (batch)\n",
    "\n",
    "                # Train the model discarting 20% of cases to avoid generalization\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "                if (debug >= 2):\n",
    "                    print (\"Model updated\")\n",
    "\n",
    "            # Evaluates model and print results\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print(\n",
    "                \"{0:.0f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}\\t {5:.0f}\\t {6:.0f}\\t {7:.0f}\\t {8:.0f}\".format(  # print a simple table\n",
    "                    iter,\n",
    "                    losses[\"textcat\"],\n",
    "                    scores[\"textcat_p\"],\n",
    "                    scores[\"textcat_r\"],\n",
    "                    scores[\"textcat_f\"],\n",
    "                    scores[\"tp\"],\n",
    "                    scores[\"fp\"],\n",
    "                    scores[\"fn\"],\n",
    "                    scores[\"tn\"]\n",
    "                )\n",
    "            )  \n",
    "            loss = losses[\"textcat\"]\n",
    "            iter = iter + 1\n",
    "            \n",
    "        print(\"False positives:\")\n",
    "        display(scores[\"fp_list\"][:10])\n",
    "        print(\"False negatives:\")\n",
    "        display(scores[\"fn_list\"][:5])\n",
    "        \n",
    "        print(\"Model trained\")    \n",
    "    \n",
    "    # test the trained model\n",
    "    print (\"\")\n",
    "    print (\"Some examples: \")\n",
    "    test_texts ={\"ca\": [\"Costa que pengi els materials acordats al campus\",\n",
    "                    \"No he tingut aquesta professora\",\n",
    "                    \"Sembla que tingui ganes de que patim, i ens tracta de tontos\",\n",
    "                    \"No ha corregit examens fets a octubre, i estem al mes de gener.\",\n",
    "                    \"No acabem d'entendre què és el que vol, però sembla que no va amb ella el tema\"],\n",
    "            \"es\": [\"No he aprendido nada en ninguna de sus clases, ni siquiera he encontrado herramientas para buscarme la vida.\", \n",
    "                   \"Fue una de las maestras más sensatas y preocupadas por los estudiantes\",\n",
    "                   \"Considero que no debería de seguir siendo profesor del inefc, puesto que ni tiene conocimientos ni ganas.\",\n",
    "                   \"Hace que los alumnos vayan con muchas ganas a sus clases\"\n",
    "                   \"Habla mal a los alumnos y se reprime a los que se quejan\"\n",
    "                  ],\n",
    "            \"en\": [\"I haven't had this professor\",\n",
    "                   \"We don't know this person\",\n",
    "                   \"He isn't a good professor\",\n",
    "                   \"We had only a few lessons\"\n",
    "                  ]\n",
    "        }     \n",
    "    for test_text in test_texts[language]:\n",
    "        doc = nlp(test_text)\n",
    "        print(test_text, doc.cats)\n",
    "    print (\"\")\n",
    "    \n",
    "    # Save de model\n",
    "    #with nlp.use_params(optimizer.averages):\n",
    "    #    nlp.to_disk(pathmodel + language)\n",
    "    #print(\"Saved model to\", pathmodel + language)\n",
    "        \n",
    "    # test the saved model\n",
    "    #print(\"Loading from\", pathmodel+ language)\n",
    "    #nlp2 = spacy.load(pathmodel +  language)\n",
    "    #for test_text in test_texts[language]:\n",
    "    #    doc2 = nlp2(test_text)\n",
    "    #    print(test_text, doc2.cats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathori = \"../data/original\"\n",
    "pathdest = \"../data/preprocessed/\"\n",
    "pathmodel = \"../data/models/model2/\"\n",
    "debug = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN MODEL\n",
      "Model loaded:  ca_fasttext_wiki_lg\n",
      "Pipes prepared\n",
      "LOAD_DATA\n",
      "Language:  ca\n",
      "Original data:  6975\n",
      "True comments:  387\n",
      "Train True data: 310 , Test True data:  77\n",
      "False comments: 6588\n",
      "Used data: 700\n",
      "Train data:  560 , Test data:  140\n",
      "Used False comments:  313\n",
      "Train False data:  250 , Test False data:  63\n",
      "Using 700 examples (560 training, 140 evaluation)\n",
      "Train_data: \n",
      "[('Allarga molt la durada de les classes', {'cats': {'POSITIVE': True, 'NEGATIVE': False}}), ('La seva actitud en vers a la Gloria es de \"pilota\" total, ja que diu que sí a tot sense saber el que ha dit ni com ho ha dit. Les seves paraules tremoles quan la Gloria està escoltant perquè sap que aquesta assignatura gira en el món de la Gloria', {'cats': {'POSITIVE': True, 'NEGATIVE': False}}), (\" és un xulo. Motius: en una videoconferencia va menjar xicle tota l'estona, i en un moment en el qual cada grup explicava lo que havia trobat sobre una feina, ell es va aixecar, es va treure els auriculars i va començar a cuinar una camamilla\", {'cats': {'POSITIVE': True, 'NEGATIVE': False}}), (\"Fa comentaris de mal gust que poden portar a discussions que no tenen res a veure amb l'assignatura, cosa que fa perdre temps i s'ha d'acabar el temari de presa i sense profunditzar. \", {'cats': {'POSITIVE': True, 'NEGATIVE': False}}), (\"També es un professor que no avalua de manera equitativa a tot l'alumnat, i te preferencies entre alumnes, cosa que trobo molt poc professional per part seva. \", {'cats': {'POSITIVE': True, 'NEGATIVE': False}})]\n",
      "Training the model...\n",
      "ITER \tLOSS \t  P  \t  R  \t  F  \t TP  \t FP  \t FN  \t TN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-6b79bd23b264>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_com[\"label\"] = data_com[\"TipusIncidencia\"] == \"Comentari problemàtic\"  # CHANGED\n",
      "<ipython-input-6-6b79bd23b264>:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_com[\"tuples\"] = data_com.apply(lambda row: (row[\"Comentari\"], {\"POSITIVE\": bool(row[\"label\"]), \"NEGATIVE\": not bool(row[\"label\"])}), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t3.758\t0.762\t0.831\t0.795\t 64\t 20\t 13\t 43\n",
      "2\t1.683\t0.791\t0.883\t0.834\t 68\t 18\t 9\t 45\n",
      "3\t0.407\t0.753\t0.909\t0.824\t 70\t 23\t 7\t 40\n",
      "4\t0.140\t0.744\t0.870\t0.802\t 67\t 23\t 10\t 40\n",
      "5\t0.048\t0.759\t0.857\t0.805\t 66\t 21\t 11\t 42\n",
      "6\t0.018\t0.750\t0.857\t0.800\t 66\t 22\t 11\t 41\n",
      "7\t0.014\t0.756\t0.883\t0.814\t 68\t 22\t 9\t 41\n",
      "8\t0.008\t0.747\t0.883\t0.810\t 68\t 23\t 9\t 40\n",
      "9\t0.001\t0.744\t0.870\t0.802\t 67\t 23\t 10\t 40\n",
      "10\t0.002\t0.753\t0.870\t0.807\t 67\t 22\t 10\t 41\n",
      "11\t0.000\t0.753\t0.870\t0.807\t 67\t 22\t 10\t 41\n",
      "12\t0.000\t0.753\t0.870\t0.807\t 67\t 22\t 10\t 41\n",
      "False positives:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Bon professor tècnicament parlant.,\n",
       " Literalment ens hem estudiat la seva part del temari a casa sols.,\n",
       " Potser fa falta més planificació sobre els temes a abordar. Tot i així els temes abordats són d'utilitat i l'ensenyament és bo. Em reitero amb la necessitat tocar altres exemples d'altres territoris de muntanya fora de Catalunya (resta de Pirineu, Euskal Herria, França, Alps, Asturies, Escandinàvia, etc..).,\n",
       " Que un professor utilitzi els apunts d’un altre professor, sense haver-los revisat es molt greu. Estan plens d’errors, que any rere any es repeteixen. Canviar arxius sense dir res i després trobar-nos amb 2 o 3 versions dels apunts amb informació diferent entre els companys complica molt l’estudi.\n",
       " Ficar una nota mínima de 5 en un examen complica bastant les coses ja que en mal dia en l’examen t’avoca a la recuperació sense cap altra possibilitat. \n",
       " Trobo correcte realitzar un treball en Cype però en cap cas i mai fer-lo en horari fora de classe i de 19-21 hores. No es pot explicar el funcionament de un programa en 4 hores es impossible. Amb aquestes tècniques només s’aconsegueix que copiem dades al programa sense entendre res del que fem i a conseqüència, no aprendre res i frustrar-nos. \n",
       " S’hauria de fer el treball al llarg del curs i així complementar les classes de teoria. \n",
       " Respecte al exercicis fets a classe, llegir d’un full i copiar el que hi diu no es ensenyar, es copiar i això ho sabem fer tots. A sobre si as tret un exercici del programa FTOOLS, es podria dignar a revisar-los ja que estan plens d’errors.\n",
       " D’altra banda cap dir que les excursions van estar be i la metodologia que utilitza per als test ajuda a estudiar.,\n",
       " No coneixem l'avaluació de l'assignatura. No tenim una metodología establerta i tampoc un seguiment de temes o casos. \n",
       " Realitzem casos i no rebem les correccions i tampoc sabem si estan bé o no, per tant no sé si l'assignatura ha contribuit a la meva formació. ,\n",
       " És el coordinador de l'assignatura però no respon a tal càrrec.,\n",
       " Bones, crec que seria profitós veure el que s'ha treballat en cada dia de classe i fer com un qüestionari curt o unes tasques curtes per interioritzar més els conceptes o com a mínim tenir un arxiu on estigui aquesta informació. Però molt bé tot en general.,\n",
       " Tarda molt a contestar els correus i ara que no fem classes presencials no ens envia correus dient si hi haurà classe o no i si ho avisa és al mateix dia. Les classes online que ens fa no em semblen el mètode més adequat per aprendre. ,\n",
       " Va ser professora virtual. Però va atendre els missatges que li vaig enviar i va intentar solucionar els meus dubtes, menys un missatge, que no el va respondre.,\n",
       " El Roger ens ha enviat el text del temario que hauria explicat a classe, a més d'algún vídeo explicatiu i els powers. També ha contestant ràpidament quan li hem preguntar alguna cosa i ha accedit a moure la data de l'examen final per tal d'ajudar-nos, ja que la situación en la que ens trobem no és gens fácil.]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[En un parell d'ocasions vam sortir a les 20.00 de classe quan havíem de sortir a les 19.00. No estic d'acord amb què ens hagin de treure temps de la nostra vida fent coses que realment podria haver donat temps de fer dins l'horari establert.,\n",
       " Un aspecte important de millora es la manera d'adreçar-se als alumnes, ja que en ocasions no ha estat del tot correcte (missatges del campus i correus).,\n",
       " La metodologia de l'assignatura de comunicació lingüística no ha estat gens ben enfocada. A més a més, salta a la primera com si fòssim nens i nenes de primària. I trobo que hi ha agut alguna falta de respecta o mala paraula/actitud cap a nosaltres. ,\n",
       " Aquest home primer de tot es passa les primeres classes explicant com funciona l'assignatura perquè és tan complicat d'entendre que ha de dedicar les primeres setmanes només a explicar el funcionament de l'assignatura. I amb això ja perdem temps d'aprenentatge. Es pensa que el funcionament és perfecte i innovador però la veritat és que tal i com ho du a terme és un autèntic caos i bogeria;,\n",
       " Crec que és molt bona persona i sempre busca el millor per l'alumnat però tot i això la seva metodologia per fer les classes deixa molt que desitjar. Els treballs són bastant absurds i no crec que resumir un power point em sigui de molta utilitat com a futura docent. També crec que la manera d'impartir les seves classes sigui la millor i també destacar el fet de que imposa massa la seva idiologia durant les classes, a vegades fent que ens sentim reprimits a l'hora de donar la nostra opinió. ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "\n",
      "Some examples: \n",
      "Costa que pengi els materials acordats al campus {'POSITIVE': 0.9999992847442627, 'NEGATIVE': 6.741091169715219e-07}\n",
      "No he tingut aquesta professora {'POSITIVE': 1.0, 'NEGATIVE': 2.7282121450866725e-08}\n",
      "Sembla que tingui ganes de que patim, i ens tracta de tontos {'POSITIVE': 0.9999921321868896, 'NEGATIVE': 7.87929275247734e-06}\n",
      "No ha corregit examens fets a octubre, i estem al mes de gener. {'POSITIVE': 0.9999998807907104, 'NEGATIVE': 1.3025432110680413e-07}\n",
      "No acabem d'entendre què és el que vol, però sembla que no va amb ella el tema {'POSITIVE': 1.0, 'NEGATIVE': 1.1511594344426612e-08}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "debug = 1\n",
    "target_loss = 0.001\n",
    "n_texts = 700\n",
    "#languages = {\"ca\":\"ca_fasttext_wiki\", \"es\":\"es_core_news_sm\", \"en\":\"en_core_web_sm\" }\n",
    "#languages = {\"ca\":\"ca_fasttext_wiki_lg\", \"es\":\"es_core_news_lg\", \"en\":\"en_core_web_sm\" }\n",
    "languages = {\"ca\":\"ca_fasttext_wiki_lg\"}\n",
    "\n",
    "for language in languages:\n",
    "    model = languages[language]\n",
    "    \n",
    "    print()\n",
    "    #print (\"TRAINING LANGUAGE: \" + language)    \n",
    "    #print (\"Model: \" + model)\n",
    "    train_model (model, language, target_loss, n_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test load_data function\n",
    "debug = 2\n",
    "n_texts = 2000\n",
    "language = \"ca\"\n",
    "model = \"ca_fasttext_wiki\"\n",
    "\n",
    "# Load the model form spacy\n",
    "nlp = spacy.load(model)\n",
    "if (debug >= 1):\n",
    "    print (\"Model loaded: \", model)\n",
    "\n",
    "# load training and test data\n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(n_texts, 0.8, language, nlp)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
