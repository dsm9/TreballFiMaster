{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Tipus incidència 'Comentari problemàtic'\n",
    "\n",
    "This notebook creates a evaluates a model to detect the issue 'Comentari problemàtic'.\n",
    "\n",
    "It will be created a model to each language: catalan, spanish and english.\n",
    "\n",
    "This type of issue is characterized by long coments composed for some sentences and imprecisse expressions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "#nlp = spacy.load(\"ca_fasttext_wiki\")\n",
    "#nlp = spacy.load(\"es_core_news_sm\")\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates tuples with texts and cats in the whole comment\n",
    "\n",
    "def create_texts_and_cats_comments(data_com, nlp):\n",
    "    \n",
    "    # Converts data frame to list\n",
    "    #data_com = data_com.dropna(subset=\"Comentari\")\n",
    "    data_com = data_com[data_com[\"Comentari\"].notnull()]\n",
    "    data_list = data_com[\"tuples\"].tolist()\n",
    "    \n",
    "    # Change order of comments\n",
    "    random.shuffle(data_list)\n",
    "    if (debug >= 2):\n",
    "        print (\"Tuples list:\")\n",
    "        print (data_list[:5])  \n",
    "        \n",
    "    # Split text and label of true cases into two lists\n",
    "    texts, cats = zip(*data_list)\n",
    "    if (debug >= 2):\n",
    "        print (\"Texts:\")\n",
    "        print (texts[0:5])\n",
    "        print (\"Cats:\")\n",
    "        print (cats[0:5])  \n",
    "\n",
    "    return (texts, cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates tuples with texts and cats, spliting sentences in the comment\n",
    "\n",
    "def create_texts_and_cats_sentences(data_com, nlp):\n",
    "    \n",
    "    # Split comments in sentences\n",
    "    data_com = data_com[data_com[\"Comentari\"].notnull()]\n",
    "    train_data = data_com[\"tuples\"].tolist()\n",
    "    train_list = []\n",
    "    for row in train_data:\n",
    "        comment = row[0]\n",
    "        label = row[1]\n",
    "        doc = nlp(comment)\n",
    "        for sent in doc.sents:\n",
    "            train_list.append((sent.text, label))\n",
    "            if ((debug >=2) and (len(train_list)<=10)):\n",
    "                print(\"+ \", sent)\n",
    "                print(\"  \", label)\n",
    " \n",
    "    if (debug >= 1):\n",
    "        print(\"Sentences: \", len(train_list))\n",
    "\n",
    "    # Change order of comments\n",
    "    random.shuffle(train_list)\n",
    "    if (debug >= 2):\n",
    "        print (\"Tuples list:\")\n",
    "        print (train_list[:5])  \n",
    "        \n",
    "     # Split text and label of true cases into two lists\n",
    "    texts, cats = zip(*train_list)\n",
    "    if (debug >= 2):\n",
    "        print (\"Texts:\")\n",
    "        print (texts[0:5])\n",
    "        print (\"Cats:\")\n",
    "        print (cats[0:5])   \n",
    "        \n",
    "    return (texts, cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads information from preprocessed file to create de train and test data\n",
    "\n",
    "def load_data(limit = 0, split = 0.8, language = \"es\", nlp = None):\n",
    "\n",
    "    if (debug >= 1):\n",
    "        print (\"LOAD_DATA\")\n",
    "        print (\"Language: \", language)\n",
    "        \n",
    "    # Load the model form spacy\n",
    "#    nlp = spacy.load(model)\n",
    "#    if (debug >= 1):\n",
    "#        print (\"Model loaded: \", model)\n",
    "    \n",
    "    # Load data from file\n",
    "    file = \"comentaris_\" + language + \".csv\"\n",
    "    data = pd.read_csv(pathdest + file)\n",
    "    if (debug >= 1):\n",
    "        print(\"Original data: \", data.shape[0])\n",
    "        \n",
    "    if (debug >= 2):\n",
    "        print (\"Sample:\")\n",
    "        display (data.sample(5))\n",
    "\n",
    "    # Calculates label    \n",
    "    data_com = data[[\"Comentari\",\"TipusIncidencia\"]]    # CHANGED\n",
    "    if (debug >= 2):\n",
    "        print (\"Filtered columns:\")\n",
    "        display (data_com.sample(5))    \n",
    "\n",
    "    # Calculates tuples row\n",
    "    # Converts: label=True -> {\"POSITIVE\": True, \"NEGATIVE\": False}\n",
    "    # label=False -> {\"POSITIVE\": False, \"NEGATIVE\": True}\n",
    "    data_com[\"label\"] = data_com[\"TipusIncidencia\"] == \"Comentari problemàtic\"  # CHANGED\n",
    "    data_com[\"tuples\"] = data_com.apply(lambda row: (row[\"Comentari\"], {\"POSITIVE\": bool(row[\"label\"]), \"NEGATIVE\": not bool(row[\"label\"])}), axis=1)    \n",
    "    if (debug >= 2):\n",
    "        print (\"Tuples dataframe:\")\n",
    "        display (data_com.sample(5))\n",
    "\n",
    "    # Create texts and cats lists for true cases\n",
    "    data_true = data_com[data_com[\"label\"]==True]\n",
    "    if (debug >= 1):\n",
    "        print (\"True comments: \", data_true.shape[0])\n",
    "        \n",
    "    (texts_true, cats_true) = create_texts_and_cats_comments(data_true, nlp)\n",
    "    #(texts_true, cats_true) = create_texts_and_cats_sentences(data_true, nlp)\n",
    "    if (debug >= 2):\n",
    "        print(\"Texts:\",len(texts_true), \"Cats:\", len(cats_true))\n",
    "        \n",
    "    # Size of train_data_true and test_data_true\n",
    "    split_true = int(len(texts_true) * split)+1\n",
    "    if (debug >= 1):\n",
    "        print (\"Train True data:\", split_true, \", Test True data: \", len(texts_true)-split_true)      \n",
    "                \n",
    "    # Create texts and cats lists for false cases        \n",
    "    data_false = data_com[data_com[\"label\"]==False]\n",
    "    if (debug >= 1):\n",
    "        print (\"False comments:\", data_false.shape[0])\n",
    "\n",
    "    (texts_false, cats_false) = create_texts_and_cats_comments(data_false, nlp)\n",
    "    #(texts_false, cats_false) = create_texts_and_cats_sentences(data_false, nlp)\n",
    "    if (debug >= 2):\n",
    "        print(\"Texts:\",len(texts_false), \"Cats:\", len(cats_false))        \n",
    "       \n",
    "    # Size of train_data and test_data, considering limit\n",
    "    if (limit > len(texts_true) + len(texts_false)):\n",
    "        limit = len(texts_true) + len(texts_false)\n",
    "    split_total = int(limit * split)\n",
    "\n",
    "    if (debug >= 1):\n",
    "        print (\"Used data:\", limit)\n",
    "        print (\"Train data: \", split_total, \", Test data: \", limit - split_total)\n",
    "         \n",
    "    # Size of train_data_false and test_data_false\n",
    "    train_split_false = split_total - split_true\n",
    "    test_split_false = (limit - split_total) - (len(texts_true) - split_true) \n",
    "        \n",
    "    if (debug >= 1):\n",
    "        print (\"Used False comments: \", limit - len(texts_true))\n",
    "        print (\"Train False data: \", train_split_false, \", Test False data: \", test_split_false)  \n",
    "        \n",
    "         \n",
    "    # Mix true and false cases and split in train and devel\n",
    "    # All the true cases are included\n",
    "    train_texts = texts_true[:split_true] + texts_false[:train_split_false]\n",
    "    train_cats = cats_true[:split_true] + cats_false[:train_split_false]\n",
    "    test_texts = texts_true[split_true:] + texts_false[-test_split_false:]\n",
    "    test_cats = cats_true[split_true:] + cats_false[-test_split_false:]\n",
    "        \n",
    "    # Return train data and test data\n",
    "    return (train_texts, train_cats), (test_texts, test_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the level of accuracy using the test set to compare the prediction with the label defined by the user \n",
    "# Obtains the indicators: Precission (p), Recall (r) and F-score (f)\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    fp_list = list()\n",
    "    fn_list = list()\n",
    "    \n",
    "    if (debug == 2):\n",
    "        print (\"EVALUATE\")\n",
    "        \n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if label == \"NEGATIVE\":\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.0\n",
    "                if (debug >= 2):\n",
    "                    print (\"tp: \", doc)\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.0\n",
    "                fp_list.append(doc)\n",
    "                if (debug >= 2):\n",
    "                    print (\"fp: \", doc)\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "                if (debug >= 2):\n",
    "                    print (\"tn: \", doc)\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "                fn_list.append(doc)\n",
    "                if (debug >= 2):\n",
    "                    print (\"fn: \", doc)   \n",
    "    if (debug >= 2):\n",
    "        print(\"tp: \", tp, \", fp: \", fp, \", tn: \", tn, \", fn: \", fn)\n",
    "        \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score, \n",
    "            \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn,\n",
    "            \"fp_list\": fp_list, \"fn_list\": fn_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the training information, transforms it, trains the model with this set of data, evaluate the result \n",
    "# and saves the resulting model\n",
    "\n",
    "def train_model(model=None, language=\"ca\", target_loss=0.001, n_texts=2000):\n",
    "    if (debug >= 1):\n",
    "        print (\"TRAIN MODEL\")\n",
    "        \n",
    "    # Load the model form spacy\n",
    "    nlp = spacy.load(model)\n",
    "    if (debug >= 1):\n",
    "        print (\"Model loaded: \", model)\n",
    "        \n",
    "    # add the text classifier to the pipeline if it doesn't exist\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    # add label to text classifier\n",
    "    textcat.add_label(\"POSITIVE\")\n",
    "    textcat.add_label(\"NEGATIVE\")\n",
    "    \n",
    "    if \"sentencizer\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "        \n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"textcat\", \"trf_wordpiecer\", \"trf_tok2vec\",\"sentencizer\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "#        if init_tok2vec is not None:\n",
    "#            with init_tok2vec.open(\"rb\") as file_:\n",
    "#                textcat.model.tok2vec.from_bytes(file_.read())        \n",
    "        \n",
    "        if (debug >= 1):\n",
    "            print (\"Pipes prepared\")\n",
    "        \n",
    "        # load training and test data\n",
    "        (train_texts, train_cats), (dev_texts, dev_cats) = load_data(n_texts, 0.8, language, nlp)        \n",
    "\n",
    "        if (debug >= 1):\n",
    "            print(\n",
    "                \"Using {} examples ({} training, {} evaluation)\".format(\n",
    "                    len(train_texts)+len(dev_texts), len(train_texts), len(dev_texts)\n",
    "                )\n",
    "            )      \n",
    "\n",
    "        # converts the data to the format:\n",
    "        # (text, {'cats': {'POSITIVE': True, 'NEGATIVE': False}}))\n",
    "        train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
    "        if (debug>=1):\n",
    "            print (\"Train_data: \")\n",
    "            print (train_data[:5])        \n",
    "    \n",
    "        # Train the model\n",
    "        print(\"Training the model...\")\n",
    "        print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\n",
    "            \"ITER\", \"LOSS\", \"P\", \"R\", \"F\",\"TP\",\"FP\",\"FN\",\"TN\"))\n",
    "        batch_sizes = compounding(4.0, 32.0, 1.001)\n",
    "        loss = 100\n",
    "        prev_loss = 1000\n",
    "        iter = 1\n",
    "#        if (loss > target_loss):\n",
    "        while ((loss > target_loss) | (prev_loss - loss > target_loss)) & (iter <= 20):\n",
    "            prev_loss = loss\n",
    "    #        for i in range(n_iter):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=batch_sizes)\n",
    "\n",
    "#            if True:\n",
    "#                batch = next(batches)\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                if (debug >= 2):\n",
    "                    print (\"Batch: \", len(batch))\n",
    "                    print (batch)\n",
    "\n",
    "                # Train the model discarting 20% of cases to avoid generalization\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "                if (debug >= 2):\n",
    "                    print (\"Model updated\")\n",
    "\n",
    "            # Evaluates model and print results\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "            print(\n",
    "                \"{0:.0f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\\t{4:.3f}\\t {5:.0f}\\t {6:.0f}\\t {7:.0f}\\t {8:.0f}\".format(  # print a simple table\n",
    "                    iter,\n",
    "                    losses[\"textcat\"],\n",
    "                    scores[\"textcat_p\"],\n",
    "                    scores[\"textcat_r\"],\n",
    "                    scores[\"textcat_f\"],\n",
    "                    scores[\"tp\"],\n",
    "                    scores[\"fp\"],\n",
    "                    scores[\"fn\"],\n",
    "                    scores[\"tn\"]\n",
    "                )\n",
    "            )  \n",
    "            loss = losses[\"textcat\"]\n",
    "            iter = iter + 1\n",
    "            \n",
    "        print(\"False positives:\")\n",
    "        display(scores[\"fp_list\"][:10])\n",
    "        print(\"False negatives:\")\n",
    "        display(scores[\"fn_list\"][:5])\n",
    "        \n",
    "        print(\"Model trained\")    \n",
    "    \n",
    "    # test the trained model\n",
    "    print (\"\")\n",
    "    print (\"Some examples: \")\n",
    "    test_texts ={\"ca\": [\"Costa que pengi els materials acordats al campus\",\n",
    "                    \"No he tingut aquesta professora\",\n",
    "                    \"Sembla que tingui ganes de que patim, i ens tracta de tontos\",\n",
    "                    \"No ha corregit examens fets a octubre, i estem al mes de gener.\",\n",
    "                    \"No acabem d'entendre què és el que vol, però sembla que no va amb ella el tema\"],\n",
    "            \"es\": [\"No he aprendido nada en ninguna de sus clases, ni siquiera he encontrado herramientas para buscarme la vida.\", \n",
    "                   \"Fue una de las maestras más sensatas y preocupadas por los estudiantes\",\n",
    "                   \"Considero que no debería de seguir siendo profesor del inefc, puesto que ni tiene conocimientos ni ganas.\",\n",
    "                   \"Hace que los alumnos vayan con muchas ganas a sus clases\"\n",
    "                   \"Habla mal a los alumnos y se reprime a los que se quejan\"\n",
    "                  ],\n",
    "            \"en\": [\"I haven't had this professor\",\n",
    "                   \"We don't know this person\",\n",
    "                   \"He isn't a good professor\",\n",
    "                   \"We had only a few lessons\"\n",
    "                  ]\n",
    "        }     \n",
    "    for test_text in test_texts[language]:\n",
    "        doc = nlp(test_text)\n",
    "        print(test_text, doc.cats)\n",
    "    print (\"\")\n",
    "    \n",
    "    # Save de model\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(pathmodel + language)\n",
    "    print(\"Saved model to\", pathmodel + language)\n",
    "        \n",
    "    # test the saved model\n",
    "    print(\"Loading from\", pathmodel+ language)\n",
    "    nlp2 = spacy.load(pathmodel +  language)\n",
    "    for test_text in test_texts[language]:\n",
    "        doc2 = nlp2(test_text)\n",
    "        print(test_text, doc2.cats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathori = \"../data/original\"\n",
    "pathdest = \"../data/preprocessed/\"\n",
    "pathmodel = \"../data/models/model2/\"\n",
    "debug = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'ca_fasttext_wiki' (0.0.2) requires spaCy v2.1 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded:  ca_fasttext_wiki\n",
      "Pipes prepared\n",
      "LOAD_DATA\n",
      "Language:  ca\n",
      "Original data:  6975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-6b79bd23b264>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_com[\"label\"] = data_com[\"TipusIncidencia\"] == \"Comentari problemàtic\"  # CHANGED\n",
      "<ipython-input-6-6b79bd23b264>:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_com[\"tuples\"] = data_com.apply(lambda row: (row[\"Comentari\"], {\"POSITIVE\": bool(row[\"label\"]), \"NEGATIVE\": not bool(row[\"label\"])}), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True comments:  387\n",
      "Train True data: 310 , Test True data:  77\n",
      "False comments: 6588\n",
      "Used data: 700\n",
      "Train data:  560 , Test data:  140\n",
      "Used False comments:  313\n",
      "Train False data:  250 , Test False data:  63\n",
      "Using 700 examples (560 training, 140 evaluation)\n",
      "Train_data: \n",
      "[(\"La Dolors Mayoral no m'ha convençut gens ni les seves classes ni el que explicava i opino que no m'han servit les seves classes. Em sap greu perquè no m'agrada dir aquestes coses així negatives perquè penso que de tot en podem treure profit i coses positives però esque les seves classes donava la sensació com si no se les preparés massa o no les fes acutals al S.XXI, any 2020 que vivim. Les famílies han evolucionat molt i canviat molt i donava la sensació que no n'era del tot conscient. Canviava de temes sovint sense cap sentit. Em sap greu però no m'ha servit molt d'aprenentatge. \", {'cats': {'POSITIVE': True, 'NEGATIVE': False}}), (\"No especifica com hem de realitzar els treballs. Tots hem anat molt perduts i l'ajuda a sigut poca per part d'ell. Penso que ens hauria de contestar mes adequadament els nostres dubtes per saber com i que hem de fer en l'assignatura. \", {'cats': {'POSITIVE': True, 'NEGATIVE': False}}), (\"Té una actitud una mica xuleta i tot i què diu que no hi hauria cap tipus de justificació per a la falta als seminaris, ni tan sols mèdica perquè hi ha altres gurps amb què es pot assisitr, deprés ell va faltr perquè no s'havia apuntat aquella classe al calendari del mòbil.\\n\", {'cats': {'POSITIVE': True, 'NEGATIVE': False}}), (\"És una assignatura que no serveix per res, des de el meu punt de vista, no pot ser que fem el cicle de l'aigua per exemple, temari que dona el meu germà petit a 2n de Primaria. A part d'això la professora és una mica inepta, no explica bé i ens xuleja i ens vacil·la als alumnes.\", {'cats': {'POSITIVE': True, 'NEGATIVE': False}}), ('No estic d\\'acord que s\\'utilitzi a gent amb diversitat funcional per a que els alumnes els \"estudiïn\" i en treguin unes conclusions', {'cats': {'POSITIVE': True, 'NEGATIVE': False}})]\n",
      "Training the model...\n",
      "ITER \tLOSS \t  P  \t  R  \t  F  \t TP  \t FP  \t FN  \t TN  \n",
      "1\t4.164\t0.762\t0.831\t0.795\t 64\t 20\t 13\t 43\n",
      "2\t1.909\t0.771\t0.831\t0.800\t 64\t 19\t 13\t 44\n",
      "3\t0.564\t0.759\t0.779\t0.769\t 60\t 19\t 17\t 44\n",
      "4\t0.132\t0.741\t0.818\t0.778\t 63\t 22\t 14\t 41\n",
      "5\t0.066\t0.750\t0.857\t0.800\t 66\t 22\t 11\t 41\n",
      "6\t0.018\t0.761\t0.870\t0.812\t 67\t 21\t 10\t 42\n",
      "7\t0.002\t0.761\t0.870\t0.812\t 67\t 21\t 10\t 42\n",
      "8\t0.004\t0.767\t0.857\t0.810\t 66\t 20\t 11\t 43\n",
      "9\t0.002\t0.761\t0.870\t0.812\t 67\t 21\t 10\t 42\n",
      "10\t0.000\t0.744\t0.870\t0.802\t 67\t 23\t 10\t 40\n",
      "11\t0.001\t0.770\t0.870\t0.817\t 67\t 20\t 10\t 43\n",
      "12\t0.000\t0.767\t0.857\t0.810\t 66\t 20\t 11\t 43\n",
      "13\t0.000\t0.767\t0.857\t0.810\t 66\t 20\t 11\t 43\n",
      "False positives:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-No em sembla bé el temps que ens han donat per a fer els exàmens tipus test, ja que crec que ha estat insuficient (30 min per a 25 preguntes). Vam tenir molts problemes per a fer el primer examen, ja que no estava ''penjat l'examen a sakai'' i vam haver d'esperar 1 hora per a poder fer-ho. A més,  internet no responia d'una manera agradable ni pràctica, es penjava la pàgina de la UDL varies vegades, no carregava o trigava massa a fer-ho, etc. \n",
       " \n",
       " -Tampoc veig bé que s'hagi tret l'examen de recuperació que hi havia inicialment de l'examen escrit, i que ara es passi a fer 3 exàmens tipus test i un treball, que valen un 25% cada un, i, per tant, no hi hagi dret a recuperació. \n",
       " \n",
       " -Els PowerPoints que ens facilita el professorat són incomplerts, bastant pobres i ens demana que els hem de completar nosaltres mateixos per a estudiar, quan això hauria de ser feina del professor/a que en circumstàncies normals s'hauria de fer a classe i no a casa.   \n",
       " \n",
       " -Estic bastant decebuda amb l'assignatura respecte la manera com s'ha dut a terme, el poc seguiment i poca informació que hi ha hagut en relació amb els alumnes i el mètode d'aprenentatge, el qual deixa molt que desitjar.   \n",
       " ,\n",
       " Victor resco, qui es aquest home?,\n",
       " No m’agrada gens la seva manera d’explicar perquè no s’entén i pregunta temari als exàmens que havia dit que no entrava, prèviament.,\n",
       " Crec que no ho enfoca del tot bé passant links de videos a classe.,\n",
       " La majoria d'aspectes son positius, peró els temes d'explicacio sobre herbicides, resistencies... hauria de esquematitzar molt millor i anar mes a poc a poc explicant, ja que es un tema bastan complicat i moltes de les idees no les pots \"agafar\" en aquell moment.,\n",
       " Com a docent trobo que és un molt bon professional ja que la tasca de transmetre'ns coneixements la desenvolupa amb èxit (jo a les seves classes aprenc molt). Tot i que un aspecte a millorar potser és que ens seria útil a l'alumnat que se'ns facilités el material de suport de classe (Power Points). ,\n",
       " Hauria de millorar el to condescendent a l'hora de contestar als alumnes, alguna vegada és bastant impertinent.\n",
       " Allarga les explicacions de temes que no corresponen al temari. \n",
       " \n",
       " Com a aspecte positiu, va ser la primera en adaptar les classes via online durant el confinament. \n",
       " ,\n",
       " Molta entrega amb els seus alumnes. És una professora 10.,\n",
       " És un docent, que no transmet ganes de rebre coneixements. El que realitzem a les videoconferències, no serveix per a res. Ja que comença a parlar i divaga molt, el to de veu que utilitza no és gens correcte, ja que el té molt baix i parla amb una veu bastant passiva. Les lectures que ens ha manat per llegir per l'examen, a més de ser moltes, cada una d'elles és molt densa, cosa que dificulta l'aprenentatge. ,\n",
       " Difícil de valorar a aquest professor ja que no dóna senyals de vida i no ens podem posar en contacte amb ell.\n",
       " Abans de començar el confinament i ja de que el coneixia d'altres assignatures puc dic que és un bon professor, però no s'està comportant de forma professional.]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[El mètode d'ensenyament molt caòtic i desordenat, almenys per explicar la teoria de forma clara i bé s'agrairia que en comptes de recomanar un totxo de llibre hi hagués unes transparències com a suport o alguna cosa per l'estil. Perquè amb el material de classe és impossible entendre cap concepte ni aprendre la matèria. \n",
       " ,\n",
       " Assignatura massa complicada com per a ser avaluats de la manera en la que ho hem estat, un 0 per aquesta assignatura tal qual com és ara mateix.,\n",
       " Voldria deixar constància de que en una classe va comprar les persones amb síndrome de Down amb el tractament per a una valvulopatia cardíaca.,\n",
       " En aquesta situació, passa bastant de nosaltres,\n",
       " \n",
       " Per altra banda, el que trobo encara més alarmant és la gestió del tema de les pràctiques de l'assignatura. Aquestes són obligatòries per aprovar i tenen un valor molt alt en la nota final. És inadmissible que els professors pretenguin que les entreguem sense pràcticament haver treballat al laboratori amb el professor de pràctiques i fent servir un programa que no hem fet servir mai en les classes de problemes.  ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "\n",
      "Some examples: \n",
      "Costa que pengi els materials acordats al campus {'POSITIVE': 0.0001561219251016155, 'NEGATIVE': 0.9998438358306885}\n",
      "No he tingut aquesta professora {'POSITIVE': 6.645199120787135e-14, 'NEGATIVE': 1.0}\n",
      "Sembla que tingui ganes de que patim, i ens tracta de tontos {'POSITIVE': 0.0002226758806500584, 'NEGATIVE': 0.9997773766517639}\n",
      "No ha corregit examens fets a octubre, i estem al mes de gener. {'POSITIVE': 1.0, 'NEGATIVE': 4.6743547699179544e-08}\n",
      "No acabem d'entendre què és el que vol, però sembla que no va amb ella el tema {'POSITIVE': 0.9989539384841919, 'NEGATIVE': 0.0010460799094289541}\n",
      "\n",
      "Saved model to ../data/models/model2/ca\n",
      "Loading from ../data/models/model2/ca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'ca_fasttext_wiki' (0.0.2) requires spaCy v2.1 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costa que pengi els materials acordats al campus {'POSITIVE': 0.0002485798904672265, 'NEGATIVE': 0.9997513890266418}\n",
      "No he tingut aquesta professora {'POSITIVE': 8.871020779933902e-13, 'NEGATIVE': 1.0}\n",
      "Sembla que tingui ganes de que patim, i ens tracta de tontos {'POSITIVE': 0.0006448380881920457, 'NEGATIVE': 0.999355137348175}\n",
      "No ha corregit examens fets a octubre, i estem al mes de gener. {'POSITIVE': 0.9999998807907104, 'NEGATIVE': 8.503580062324545e-08}\n",
      "No acabem d'entendre què és el que vol, però sembla que no va amb ella el tema {'POSITIVE': 0.998997151851654, 'NEGATIVE': 0.0010028547840192914}\n"
     ]
    }
   ],
   "source": [
    "debug = 1\n",
    "target_loss = 0.001\n",
    "n_texts = 700\n",
    "#languages = {\"ca\":\"ca_fasttext_wiki\", \"es\":\"es_core_news_sm\", \"en\":\"en_core_web_sm\" }\n",
    "#languages = {\"ca\":\"ca_fasttext_wiki_lg\", \"es\":\"es_core_news_lg\", \"en\":\"en_core_web_sm\" }\n",
    "languages = {\"ca\":\"ca_fasttext_wiki\"}\n",
    "\n",
    "for language in languages:\n",
    "    model = languages[language]\n",
    "    \n",
    "    print()\n",
    "    #print (\"TRAINING LANGUAGE: \" + language)    \n",
    "    #print (\"Model: \" + model)\n",
    "    train_model (model, language, target_loss, n_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test load_data function\n",
    "debug = 2\n",
    "n_texts = 2000\n",
    "language = \"ca\"\n",
    "model = \"ca_fasttext_wiki\"\n",
    "\n",
    "# Load the model form spacy\n",
    "nlp = spacy.load(model)\n",
    "if (debug >= 1):\n",
    "    print (\"Model loaded: \", model)\n",
    "\n",
    "# load training and test data\n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(n_texts, 0.8, language, nlp)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
